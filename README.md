<p align="center">
  <img src="repo_files/dark_logo_banner.png" width="1000"/>
  <br>
  <em>A Case Study in E-commerce Sales & Returns Analysis</em>
</p>

<p align="center">
  <img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue">
  <img alt="Status" src="https://img.shields.io/badge/status-complete-brightgreen">
  <img alt="Python" src="https://img.shields.io/badge/python-3.9-blue.svg">
  <img alt="DB" src="https://img.shields.io/badge/database-SQLite-orange.svg">
</p>

## 1. The Challenge

This project is an analysis initiated in response to a simulated request from a VP of Sales. The primary objective was to conduct a deep-dive diagnostic on a year's worth of sales and returns data to identify key drivers of revenue loss and customer dissatisfaction.

The core business questions were:
- What are the primary drivers of our high product return rate?
- Which products, customer segments, and channels are most and least profitable?
- How can we create an actionable framework to monitor product quality and mitigate future losses?

> ğŸ’¡ **Note:** This analysis was performed on a synthetic dataset generated by the [`ecom_sales_data_generator (v0.1.0)`](https://github.com/G-Schumacher44/ecom_sales_data_generator) to simulate realistic e-commerce transactions and behaviors.

## 2. The Solution & Key Findings

I developed an end-to-end analysis pipeline using SQL for data extraction and aggregation, and Python (Pandas, Plotly) for in-depth analysis and visualization. The results were delivered through an executive-level Jupyter Notebook, a summary PDF, and an interactive dashboard.

**Key Findings:**
- **Systemic Return Issues:** All sales regions exhibited return rates exceeding 20%, indicating a systemic issue rather than a localized one.
- **High-Risk Channels:** The `NewEgg` sales channel showed an unsustainable **36% refund rate**, representing a significant margin drain.
- **Product Quality Blind Spots:** A new **Product Quality Risk Flag System** was developed, which identified that "defective" or "damaged" items accounted for over **$4.5M in refunds**.
- **High-Value Customer Paradox:** The most loyal "Platinum" customers were also among the highest-volume returners, suggesting a need to re-evaluate loyalty program incentives.

## 3. Live Demo & Deliverables

The complete analysis and findings are available in the following formats:

*   **ğŸ“Š Interactive Dashboard:** [`View the Live Dashboard on Looker Studio`](https://lookerstudio.google.com/reporting/e5f1454c-c8e4-481f-9ac8-375a3bdd289c)
    *   **ğŸ“„ PDF Looker View:** A static look at the dashboard can be found in [`reports/Sales_Diagnostic.pdf`](reports/Sales_Diagnostic.pdf).
*   **ğŸ““ Jupyter Notebook:** The full, detailed analysis can be explored in [`Executive_Retail_Returns_Report.ipynb`](Executive_Retail_Returns_Report.ipynb).
*   **ğŸ“ Business Brief:** The original stakeholder request and context can be found in [`vp_request.md`](vp_request.md).

## 4. Tech Stack & Methodology

*   **Languages:** Python, SQL
*   **Database:** SQLite (`ecom_retailer.db`)
*   **Core Libraries:** Pandas, Plotly, SQLAlchemy, `gspread`
*   **Tools:** Jupyter Notebook, VS Code, Git, Looker, Google Sheets

The project followed these steps:
1.  **Data Extraction & View Creation:** SQL scripts in `sql_sessions/` were used to query the database and create aggregated views for analysis.
2.  **Analysis & Visualization:** The Jupyter Notebook was used to perform deeper analysis, generate insights, and create visualizations with Plotly.
3.  **Data Export:** Key dataframes were exported to CSV and Excel, which power the Looker Studio dashboard.

<details>
<summary>âš™ï¸ Project Structure</summary>

# ğŸ“‚ Repository Map

```text
.
â”œâ”€â”€ output_data/                     # Exported analysis or pipeline outputs
â”œâ”€â”€ repo_files/                      # Project assets (branding, banners, etc.)
â”œâ”€â”€ reports/                         # Generated reports and summaries
â”œâ”€â”€ scripts/                         # Utility and pipeline scripts
â”‚   â”œâ”€â”€ check_db.py                  # Validate SQLite database schema
â”‚   â”œâ”€â”€ csv_to_xlsx.py               # Convert CSV exports to Excel format
â”‚   â””â”€â”€ g_drive_uploader.py          # Upload results to Google Drive/Sheets
â”œâ”€â”€ sql_sessions/                    # SQL build + analysis sessions
â”‚   â”œâ”€â”€ build_all_dashboards.sql     # Master script to create dashboards
â”‚   â”œâ”€â”€ cleanup_vp_req.sql           # Cleanup queries for VP request workflow
â”‚   â”œâ”€â”€ eda_cleaning.session.sql     # SQL session: cleaning
â”‚   â”œâ”€â”€ eda_core_metrics.session.sql # SQL session: core metrics
â”‚   â”œâ”€â”€ eda_logistics_summary.session.sql # SQL session: logistics summary
â”‚   â”œâ”€â”€ eda_segementation.session.sql     # SQL session: segmentation analysis
â”‚   â”œâ”€â”€ export_cleaned_tables.sql    # Export cleaned tables
â”‚   â”œâ”€â”€ export_views.sql             # Export final views
â”‚   â””â”€â”€ run_all.sh                   # Bash script to run all SQL sessions
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”œâ”€â”€ ecom_retailer.db                 # Pre-built SQLite database
â”œâ”€â”€ Executive_Retail_Returns_Report.ipynb # Jupyter notebook: executive analysis
â”œâ”€â”€ README.md                        # Main project introduction
â”œâ”€â”€ run_story.sh                     # Master script to run pipeline
â”œâ”€â”€ secrets.yaml                     # Secrets configuration (local use)
â”œâ”€â”€ stories_config.yaml              # Config for story-specific pipelines
â”œâ”€â”€ USAGE.md                         # Usage guide / documentation
â””â”€â”€ vp_request.md                    # VP request case notes
```

</details>

___

## 5. How to Run This Project

To get started with the project, install dependencies using one of the following methods:

**Option 1: Conda (Recommended)**
Use the full environment specification:
```bash
conda env create -f environment.yml
conda activate sql_stories
```
**Option 2: pip install**

```bash
pip install -r requirements.txt
```

ğŸ“¦ The environment.yml includes tools for working with Jupyter, SQLite, Google Sheets, and testing â€” ideal for full local development.
___

For instructions on running the automated data pipeline to refresh the dashboard data, please see the **Usage Guide**.

## ğŸ¤ On Generative AI Use

Generative AI tools (Gemini 2.5-PRO, ChatGPT 4o - 4.1) were used throughout this project as part of an integrated workflow â€” supporting code generation, documentation refinement, and idea testing. These tools accelerated development, but the logic, structure, and documentation reflect intentional, human-led design. This repository reflects a collaborative process: where automation supports clarity, and iteration deepens understanding.

---

## ğŸ“œ Licensing

This project is licensed under the MIT License.