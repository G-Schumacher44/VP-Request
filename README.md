<p align="center">
  <img src="repo_files/dark_logo_banner.png" width="1000"/>
  <br>
  <em>A Case Study in E-commerce Sales & Returns Analysis</em>
</p>

<p align="center">
  <img alt="MIT License" src="https://img.shields.io/badge/license-MIT-blue">
  <img alt="Status" src="https://img.shields.io/badge/status-complete-brightgreen">
  <img alt="Python" src="https://img.shields.io/badge/python-3.9-blue.svg">
  <img alt="DB" src="https://img.shields.io/badge/database-SQLite-orange.svg">
</p>

## 1. The Challenge

This project is an analysis initiated in response to a simulated request from a VP of Sales. The primary objective was to conduct a deep-dive diagnostic on a year's worth of sales and returns data to identify key drivers of revenue loss and customer dissatisfaction.

The core business questions were:
- What are the primary drivers of our high product return rate?
- Which products, customer segments, and channels are most and least profitable?
- How can we create an actionable framework to monitor product quality and mitigate future losses?

> 💡 **Note:** This analysis was performed on a synthetic dataset generated by the [`ecom_sales_data_generator (v0.1.0)`](https://github.com/G-Schumacher44/ecom_sales_data_generator) to simulate realistic e-commerce transactions and behaviors.

## 2. The Solution & Key Findings

I developed an end-to-end analysis pipeline using SQL for data extraction and aggregation, and Python (Pandas, Plotly) for in-depth analysis and visualization. The results were delivered through an executive-level Jupyter Notebook, a summary PDF, and an interactive dashboard.

**Key Findings:**
- **Systemic Return Issues:** All sales regions exhibited return rates exceeding 20%, indicating a systemic issue rather than a localized one.
- **High-Risk Channels:** The `NewEgg` sales channel showed an unsustainable **36% refund rate**, representing a significant margin drain.
- **Product Quality Blind Spots:** A new **Product Quality Risk Flag System** was developed, which identified that "defective" or "damaged" items accounted for over **$4.5M in refunds**.
- **High-Value Customer Paradox:** The most loyal "Platinum" customers were also among the highest-volume returners, suggesting a need to re-evaluate loyalty program incentives.

## 3. Live Demo & Deliverables

The complete analysis and findings are available in the following formats:

*   **📊 Interactive Dashboard:** [`View the Live Dashboard on Looker Studio`](https://lookerstudio.google.com/reporting/e5f1454c-c8e4-481f-9ac8-375a3bdd289c)
    *   **📄 PDF Looker View:** A static look at the dashboard can be found in [`reports/Sales_Diagnostic.pdf`](reports/Sales_Diagnostic.pdf).
*   **📓 Jupyter Notebook:** The full, detailed analysis can be explored in [`Executive_Retail_Returns_Report.ipynb`](Executive_Retail_Returns_Report.ipynb).
*   **📝 Business Brief:** The original stakeholder request and context can be found in [`vp_request.md`](vp_request.md).

## 4. Tech Stack & Methodology

*   **Languages:** Python, SQL
*   **Database:** SQLite (`ecom_retailer.db`)
*   **Core Libraries:** Pandas, Plotly, SQLAlchemy, `gspread`
*   **Tools:** Jupyter Notebook, VS Code, Git, Looker, Google Sheets

The project followed these steps:
1.  **Data Extraction & View Creation:** SQL scripts in `sql_sessions/` were used to query the database and create aggregated views for analysis.
2.  **Analysis & Visualization:** The Jupyter Notebook was used to perform deeper analysis, generate insights, and create visualizations with Plotly.
3.  **Data Export:** Key dataframes were exported to CSV and Excel, which power the Looker Studio dashboard.

<details>
<summary>⚙️ Project Structure</summary>

# 📂 Repository Map

```text
.
├── output_data/                     # Exported analysis or pipeline outputs
├── repo_files/                      # Project assets (branding, banners, etc.)
├── reports/                         # Generated reports and summaries
├── scripts/                         # Utility and pipeline scripts
│   ├── check_db.py                  # Validate SQLite database schema
│   ├── csv_to_xlsx.py               # Convert CSV exports to Excel format
│   └── g_drive_uploader.py          # Upload results to Google Drive/Sheets
├── sql_sessions/                    # SQL build + analysis sessions
│   ├── build_all_dashboards.sql     # Master script to create dashboards
│   ├── cleanup_vp_req.sql           # Cleanup queries for VP request workflow
│   ├── eda_cleaning.session.sql     # SQL session: cleaning
│   ├── eda_core_metrics.session.sql # SQL session: core metrics
│   ├── eda_logistics_summary.session.sql # SQL session: logistics summary
│   ├── eda_segementation.session.sql     # SQL session: segmentation analysis
│   ├── export_cleaned_tables.sql    # Export cleaned tables
│   ├── export_views.sql             # Export final views
│   └── run_all.sh                   # Bash script to run all SQL sessions
├── .gitattributes
├── .gitignore
├── ecom_retailer.db                 # Pre-built SQLite database
├── Executive_Retail_Returns_Report.ipynb # Jupyter notebook: executive analysis
├── README.md                        # Main project introduction
├── run_story.sh                     # Master script to run pipeline
├── secrets.yaml                     # Secrets configuration (local use)
├── stories_config.yaml              # Config for story-specific pipelines
├── USAGE.md                         # Usage guide / documentation
└── vp_request.md                    # VP request case notes
```

</details>

___

## 5. How to Run This Project

To get started with the project, install dependencies using one of the following methods:

**Option 1: Conda (Recommended)**
Use the full environment specification:
```bash
conda env create -f environment.yml
conda activate sql_stories
```
**Option 2: pip install**

```bash
pip install -r requirements.txt
```

📦 The environment.yml includes tools for working with Jupyter, SQLite, Google Sheets, and testing — ideal for full local development.
___

For instructions on running the automated data pipeline to refresh the dashboard data, please see the **Usage Guide**.

## 🤝 On Generative AI Use

Generative AI tools (Gemini 2.5-PRO, ChatGPT 4o - 4.1) were used throughout this project as part of an integrated workflow — supporting code generation, documentation refinement, and idea testing. These tools accelerated development, but the logic, structure, and documentation reflect intentional, human-led design. This repository reflects a collaborative process: where automation supports clarity, and iteration deepens understanding.

---

## 📜 Licensing

This project is licensed under the MIT License.